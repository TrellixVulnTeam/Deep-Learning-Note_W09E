# 迁移学习概述

[TOC]

## 1 为什么我们需要迁移学习

简单的来说，是让机器拥有举一反三的能力，在目标类似的情况下，避免重新训练一个新的网络。

迁移学习的优点在于：

1. 减少了对**数据**集的要求，通过迁移学习修改的网络不像传统的网络一样需要大量的数据集才能得到一个较好的结果和精度。
2. 网络的适应性更强，能适用于更多的领域。
3. 目标域的数据不需要标签，可以实现无监督学习。

## 2 基本概念

### 2.1 域

源域 $\rightarrow$ 目标域

迁移学习要做的就是把一个网络从源域迁移到目标域上。

### 2.2 共同性

迁移学习可以发现源域和目标域之间的共同性。

### 2.3 公式

#### 传统的深度学习 Traditional deep learning

$$
\min \frac{1}{n} \sum_{i=1}^{n} L\left(x_{i}, y_{i}, \theta\right)
$$

#### 特征的适应 Feature adaptation

$$
\min \frac{1}{n} \sum_{i=1}^{n} L\left(\phi\left(\mathbf{x}_{i}^{s}\right), \mathbf{y}_{i}^{s}, \theta\right)
$$

即尽量将源域和特征域的**特征提取**到同一个特征空间里，让他们对齐，以得到更好的性能

也是**最常用的方法**，其他的适应方法基本会变为小技巧出现在论文里。

#### 实例的适应 Instance adaptation

$$
\min \frac{1}{n} \sum_{i=1}^{n} w_{i} L\left(\mathbf{x}_{i}^{s}, y_{i}^{s}, \theta\right)
$$

源域和目标域肯定存在一些**样本是类似的**（比如说数据集里的图片是类似的），那么我们在训练的时候就尽量将这些样本的**权重调大**。

#### 模型的适应 Model adaptation

也是**参数**的适应
$$
\min \frac{1}{n} \sum_{i=1}^{n} L\left(\mathbf{x}_{i}^{s}, y_{i}^{s}, \theta\right)
$$

### 2.4 MMD loss

#### MMD 的基本概念

MMD（Maximum Mean Discrepancy）是迁移学习，尤其是域适应（Domain adaptation）中使用最广泛的一种**损失函数**，主要用来度量两个不同但相关的分布的距离。
$$
M M D(X, Y)=\left\|\frac{1}{n} \sum_{i=1}^{n} \phi\left(x_{i}\right)-\frac{1}{m} \sum_{j=1}^{m} \phi\left(y_{j}\right)\right\|_{H}^{2}
$$
其中 H 表示这个距离是由 $\phi()$ 将数据映射到再生希尔伯特空间（RKHS）中进行度量的。

#### 为什么要用 MMD

Domain adaptation 的目的是将源域（Source domain）中学到的知识可以应用到不同但相关的目标域（Target domain）。

本质上是要找到一个**变换函数**，使得变换后的源域数据和目标域数据的距离是最小的。

所以这其中就要涉及如何度量两个域中**数据分布差异**的问题，因此也就用到了MMD。
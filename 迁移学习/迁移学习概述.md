# 迁移学习概述

[TOC]

## 1 为什么我们需要迁移学习

简单的来说，是让机器拥有举一反三的能力，在目标类似的情况下，避免重新训练一个新的网络

迁移学习的优点在于：

1. 减少了对**数据**集的要求，通过迁移学习修改的网络不像传统的网络一样需要大量的数据集才能得到一个较好的结果和精度
2. 网络的适应性更强，能适用于更多的领域

## 2 基本概念

### 2.1 域

源域 $\rightarrow$ 目标域

迁移学习要做的就是把一个网络从源域迁移到目标域上

### 2.2 共同性

迁移学习可以发现源域和目标域之间的共同性

### 2.3 公式

#### 传统的深度学习 Traditional deep learning

$$
\min \frac{1}{n} \sum_{i=1}^{n} L\left(x_{i}, y_{i}, \theta\right)
$$

#### 特征的适应 Feature adaptation

$$
\min \frac{1}{n} \sum_{i=1}^{n} L\left(\phi\left(\mathbf{x}_{i}^{s}\right), \mathbf{y}_{i}^{s}, \theta\right)
$$

即尽量将源域和特征域的**特征提取**到同一个特征空间里，让他们对齐，以得到更好的性能

也是**最常用的方法**，其他的适应方法基本会变为小技巧出现在论文里

#### 实例的适应 Instance adaptation

$$
\min \frac{1}{n} \sum_{i=1}^{n} w_{i} L\left(\mathbf{x}_{i}^{s}, y_{i}^{s}, \theta\right)
$$

源域和目标域肯定存在一些**样本是类似的**（比如说数据集里的图片是类似的），那么我们在训练的时候就尽量将这些样本的**权重调大**

#### 模型的适应 Model adaptation

也是**参数**的适应
$$
\min \frac{1}{n} \sum_{i=1}^{n} L\left(\mathbf{x}_{i}^{s}, y_{i}^{s}, \theta\right)
$$
